01.浏览器
    一个网页加载的全过程
    1.服务器端渲染
        html的内容和数据在服务器进行整合
        在浏览器端看到的页面源代码中 有你需要的数据
    2.客户端（浏览器）渲染
        html的内容和数据在客户端进行整合是发生在浏览器上的
        这个过程一般通过脚本来完成（JavaScript）
    我们通过浏览器可以看到上述加载过程
    勾选保留日志

02.requests -> 伪装成浏览器的样子
    爬虫究竟是什么？本质是用程序模拟浏览器上的请求操作
    爬虫程序不是人 很可能衍生成病毒
    requests模块是python的一个非常完整的一个第三方模块
    专门用来模拟浏览器进行网络请求的发送
    # 发送get请求
    requests.get()
    params是专门给get请求增加参数的
    # 发送post请求
    requests.post()
    data是专门给post请求增加参数的

    get/post 是http的请求方式 我们只需要根据抓包里面的状况去选择使用
    get和post的传参方式是不一样的
    get的参数 最终都会被添加到

    状态码（HTTP系列的状态码）
    200 系列
        一般指的是你当前本次与服务器进行通信没有问题
    300 系列
        一般指的是重定向 在响应头上能看到location字样
        我们写爬虫的时候 基本上不用管302 因为requests可以自动帮你完成这个重定向的动作
    403 系列
        一般都是被风控拦截了
    404 系列
        走丢了 你的url不存在 在服务器上找不到你想要的内容
    500 系列
        表示服务器内部出现了错误
        使用浏览器访问没有问题 但是自己写的程序访问会返回500 基本上是给的参数有问题 让服务器无法正常工作了

03.简单的接触一些反爬（header）
    请求头
        User-Agent 用户用什么设备发送的请求 直接复制粘贴就好
        Cookies 服务器记录在浏览器上的一个字符串 写入在一个本地文件中的
                作用：和服务器之间保持住会话
                这个在服务器端的名称叫session
                获取办法：
                    1、直接在浏览器抓包中复制（某些网站就可以）
                    2、使用requests.session()来保持会话
                    注意：使用requests.session() 可以帮你处理set_cookies中的内容
                         但是不能帮你处理JavaScript处理的cookies的内容
                         如果网站是用js来处理cookies的 那么要自己写代码来处理这个逻辑
        Referer
            用来检测上一个url是什么 直接复制粘贴即可
        网页自己添加的一些参数（这是需要逆向的地方）
    响应头
        Location
            302重定向地址（我们不管）
        Set-Cookie
            session自动维护 我们不管
        网页自己添加的一些参数
            一般情况下是不用管的

04.对数据进行解析
    html
        re
            在html中获取到js的一部分代码（字符串）
        xpath
            用来解析常规的html网页
            etree的xpath默认返回的是列表 所以要加一个判断
            if ret:
                ret[0]
            else:
                # 做一些什么操作
        bs4
            在解析xml和svg的时候比xpath好用
    json
        res.json()
        json.loads(res.text)
        如果遇到反爬 你很可能拿到的东西和抓包工具不一致
        在收到的数据不是json的情况下 使用上面的方法会直接报错
        切记 先打印res.text 确定好收到的内容格式是json 才开始转化
    jsonp
        xxx({json})
        想办法去掉xxx 得到的就是json

05.多任务异步爬虫 多进程 多线程 协程（最麻烦的）
    进程是资源单位 进程与进程之间是相互隔离的
    定义的全局变量在每个进程中都有自己的一份 改A进程的全局变量不会影响B进程
    多进程是多了好多个程序 而多线程是在一个程序里同时跑多个子程序
    进程之间的通信可以用队列 也可以用redis

06.数据库（MySQL Mongodb redis）
    csv文件 本质上就是个文本文件
    mysql
        用native创建表格
        增加数据
            insert into 表(字段1, 字段2, 字段3...) values (值1, 值2, 值3...)
        删除数据
            delete from 表 where 条件
            删除的时候没有直接删除的 都是创建一个字段比如disable 实现软删除 在查询的时候不查询disable为1的就好了
        修改数据
            update 表 set 字段=值, 字段=值 where条件
        查询数据
            select * from 表
            select * from 表 where 条件
        python 操作 mysql
    redis
        redis存储数据的规则
            key -> 数据（单一的字符串 列表 字典 set）
            所有的命令都是根据数据类型和key来的
            基本上所有的set集合都是无序的
            zset能做到排序是因为加了权重

07.面向对象（基础）
    核心是编程思想的转变
    目前 我们写代码的思想是面向过程的思想
    1、获取页面源代码
    2、解析页面源代码
    3、去存储数据
    面向过程的步骤中 某一步出现了问题 整个程序都会崩溃
    面向对象：
        你要操纵对象 让对象给你干活
    继承：
        子类继承父类 子类自动拥有父类中的所有内容（私有的除外）


